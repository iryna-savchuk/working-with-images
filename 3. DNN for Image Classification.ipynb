{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43dfa282",
   "metadata": {},
   "source": [
    "**Description**\n",
    "\n",
    "This notebook is created to understand how Deep Neural Networks (DNNs) can be used for image classification problems. Though this method is NOT RECOMMENDED, it is possible to use DNNs to classify images. \n",
    "\n",
    "**Dataset**\n",
    "\n",
    "For the demo, the MNIST datasest will be used to train the DNN (this dataset is commonly used to train image processing systems).\n",
    "\n",
    "MNIST is handwritten digits database, which contains large quantity of handwritten digits from 0 to 9. Each digit is in grayscale, so we will be dealing with a single channel images. Every image in the dataset is standardized to be 28 * 28 pixels, giving 784 pixels for each image. \n",
    "\n",
    "The MNIST handwritten digits dataset is available here:\n",
    "http://yann.lecun.com/exdb/mnist and https://www.kaggle.com/datasets/oddrationale/mnist-in-csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6413db",
   "metadata": {},
   "source": [
    "**Parameters Explosion:**\n",
    "   - Consider a 100 * 100 pixel image (10,000 pixels per image) -->> the first layer will contain 10,000 neurons.\n",
    "   - Interconnections between this layer and the next layer ~O(10,000 * 10,000)\n",
    "   - Now, we would already have 100 million parameters to train a fully-connected neural network!\n",
    "   - The time needed to train so many parameters will be huge\n",
    "   \n",
    "Thus, dense, fully connected neural networks can't really cope with image data.\n",
    "Also, they do not provide feature extraction with locatin invariance. DNNs do not consider the spacial aspects of images. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727f1d46",
   "metadata": {},
   "source": [
    "### Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49ed3d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46b9c1d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 785), (10000, 785))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_train = pd.read_csv('datasets/mnist-in-csv/mnist_train.csv')\n",
    "mnist_test = pd.read_csv('datasets/mnist-in-csv/mnist_test.csv')\n",
    "\n",
    "mnist_train.shape, mnist_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbebe920",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>1x1</th>\n",
       "      <th>1x2</th>\n",
       "      <th>1x3</th>\n",
       "      <th>1x4</th>\n",
       "      <th>1x5</th>\n",
       "      <th>1x6</th>\n",
       "      <th>1x7</th>\n",
       "      <th>1x8</th>\n",
       "      <th>1x9</th>\n",
       "      <th>...</th>\n",
       "      <th>28x19</th>\n",
       "      <th>28x20</th>\n",
       "      <th>28x21</th>\n",
       "      <th>28x22</th>\n",
       "      <th>28x23</th>\n",
       "      <th>28x24</th>\n",
       "      <th>28x25</th>\n",
       "      <th>28x26</th>\n",
       "      <th>28x27</th>\n",
       "      <th>28x28</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  1x1  1x2  1x3  1x4  1x5  1x6  1x7  1x8  1x9  ...  28x19  28x20  \\\n",
       "0      5    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "1      0    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "2      4    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "3      1    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "4      9    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "\n",
       "   28x21  28x22  28x23  28x24  28x25  28x26  28x27  28x28  \n",
       "0      0      0      0      0      0      0      0      0  \n",
       "1      0      0      0      0      0      0      0      0  \n",
       "2      0      0      0      0      0      0      0      0  \n",
       "3      0      0      0      0      0      0      0      0  \n",
       "4      0      0      0      0      0      0      0      0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63611f52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 784)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's have a look at one image from the dataset\n",
    "img = mnist_train[1:2]\n",
    "img = img.drop('label', axis=1)\n",
    "img = img.values\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c77ac7b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 28, 28)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reshape img to be a single-channeled image\n",
    "img = img.reshape(1, 28, 28)\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5811eeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = img.squeeze() # quickly eliminating the dimension that just have single values\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6ffd3c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 600x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Displaying the image with Matplotlib\n",
    "plt.figure(figsize= (6, 6))\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "742b6f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train = mnist_train.dropna()\n",
    "mnist_test = mnist_test.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c6c10ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train_features = mnist_train.drop('label', axis=1)\n",
    "mnist_train_target = mnist_train['label']\n",
    "\n",
    "mnist_test_features = mnist_test.drop('label', axis=1)\n",
    "mnist_test_target = mnist_test['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d58c86b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train max -  255\n",
      "train min -  0\n",
      "test max -  255\n",
      "test min -  0\n"
     ]
    }
   ],
   "source": [
    "print(\"train max - \", mnist_train_features.values.max())\n",
    "print(\"train min - \", mnist_train_features.values.min())\n",
    "print(\"test max - \", mnist_test_features.values.max())\n",
    "print(\"test min - \", mnist_test_features.values.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d031dbfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train max -  1.0\n",
      "train min -  0.0\n",
      "test max -  1.0\n",
      "test min -  0.0\n"
     ]
    }
   ],
   "source": [
    "mnist_train_features = mnist_train_features.astype('float32')\n",
    "mnist_train_features = mnist_train_features/255\n",
    "mnist_test_features = mnist_test_features.astype('float32')\n",
    "mnist_test_features = mnist_test_features/255\n",
    "\n",
    "print(\"train max - \", mnist_train_features.values.max())\n",
    "print(\"train min - \", mnist_train_features.values.min())\n",
    "print(\"test max - \", mnist_test_features.values.max())\n",
    "print(\"test min - \", mnist_test_features.values.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "26380e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating torch tensors that will be used in DNN training process\n",
    "X_train_tensor = torch.tensor(mnist_train_features.values, dtype=torch.float)\n",
    "X_test_tensor = torch.tensor(mnist_test_features.values, dtype=torch.float)\n",
    "\n",
    "Y_train_tensor = torch.tensor(mnist_train_target.values, dtype=torch.long)\n",
    "Y_test_tensor = torch.tensor(mnist_test_target.values, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0fe36dbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([60000, 784]), torch.Size([60000]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tensor.shape, Y_train_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d9d37f29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10000, 784]), torch.Size([10000]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_tensor.shape, Y_test_tensor.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26691c8f",
   "metadata": {},
   "source": [
    "### Setting up a Fully Connected NN for image classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ac78ae06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d9705f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 784\n",
    "output_size = 10\n",
    "\n",
    "hidden1_size = 16\n",
    "hidden2_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2f96aa2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden1_size)\n",
    "        self.fc2 = nn.Linear(hidden1_size, hidden2_size)\n",
    "        self.fc3 = nn.Linear(hidden2_size, output_size)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.sigmoid(self.fc1(x))\n",
    "        x = torch.sigmoid(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return torch.log_softmax(x, dim=-1)  # equivalent to log(softmax()) and used with NLLLoss function\n",
    "                                             # stable numerically and has better performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52b8dbb",
   "metadata": {},
   "source": [
    "### Training the Fully Connected NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "66ba33a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a15e77da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3ab6b202",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (fc1): Linear(in_features=784, out_features=16, bias=True)\n",
       "  (fc2): Linear(in_features=16, out_features=32, bias=True)\n",
       "  (fc3): Linear(in_features=32, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "92783753",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor = X_train_tensor.to(device)\n",
    "X_test_tensor = X_test_tensor.to(device)\n",
    "Y_train_tensor = Y_train_tensor.to(device)\n",
    "Y_test_tensor = Y_test_tensor.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a570a69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "loss_fn = nn.NLLLoss()\n",
    "epochs = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "297be588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch - 10, loss - 2.29 \n",
      "Epoch - 20, loss - 2.27 \n",
      "Epoch - 30, loss - 2.25 \n",
      "Epoch - 40, loss - 2.22 \n",
      "Epoch - 50, loss - 2.18 \n",
      "Epoch - 60, loss - 2.13 \n",
      "Epoch - 70, loss - 2.08 \n",
      "Epoch - 80, loss - 2.02 \n",
      "Epoch - 90, loss - 1.95 \n",
      "Epoch - 100, loss - 1.88 \n",
      "Epoch - 110, loss - 1.81 \n",
      "Epoch - 120, loss - 1.74 \n",
      "Epoch - 130, loss - 1.68 \n",
      "Epoch - 140, loss - 1.61 \n",
      "Epoch - 150, loss - 1.56 \n",
      "Epoch - 160, loss - 1.50 \n",
      "Epoch - 170, loss - 1.45 \n",
      "Epoch - 180, loss - 1.40 \n",
      "Epoch - 190, loss - 1.35 \n",
      "Epoch - 200, loss - 1.30 \n",
      "Epoch - 210, loss - 1.26 \n",
      "Epoch - 220, loss - 1.22 \n",
      "Epoch - 230, loss - 1.18 \n",
      "Epoch - 240, loss - 1.14 \n",
      "Epoch - 250, loss - 1.11 \n",
      "Epoch - 260, loss - 1.08 \n",
      "Epoch - 270, loss - 1.04 \n",
      "Epoch - 280, loss - 1.02 \n",
      "Epoch - 290, loss - 0.99 \n",
      "Epoch - 300, loss - 0.96 \n",
      "Epoch - 310, loss - 0.94 \n",
      "Epoch - 320, loss - 0.91 \n",
      "Epoch - 330, loss - 0.89 \n",
      "Epoch - 340, loss - 0.87 \n",
      "Epoch - 350, loss - 0.85 \n",
      "Epoch - 360, loss - 0.83 \n",
      "Epoch - 370, loss - 0.81 \n",
      "Epoch - 380, loss - 0.79 \n",
      "Epoch - 390, loss - 0.77 \n",
      "Epoch - 400, loss - 0.75 \n",
      "Epoch - 410, loss - 0.73 \n",
      "Epoch - 420, loss - 0.72 \n",
      "Epoch - 430, loss - 0.70 \n",
      "Epoch - 440, loss - 0.68 \n",
      "Epoch - 450, loss - 0.67 \n",
      "Epoch - 460, loss - 0.65 \n",
      "Epoch - 470, loss - 0.64 \n",
      "Epoch - 480, loss - 0.62 \n",
      "Epoch - 490, loss - 0.61 \n",
      "Epoch - 500, loss - 0.59 \n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, epochs+1):\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    Y_pred = model(X_train_tensor)\n",
    "    \n",
    "    loss = loss_fn(Y_pred, Y_train_tensor)\n",
    "    loss.backward() # compute gradients\n",
    "    \n",
    "    optimizer.step() # updating the model parameters\n",
    "    \n",
    "    if epoch % 10 ==0:\n",
    "        print('Epoch - %d, loss - %0.2f ' %(epoch, loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd4efb9",
   "metadata": {},
   "source": [
    "### Making predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "29abc1bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (fc1): Linear(in_features=784, out_features=16, bias=True)\n",
       "  (fc2): Linear(in_features=16, out_features=32, bias=True)\n",
       "  (fc3): Linear(in_features=32, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval() # layers and operations that should be applied only in training data (e.g. dropout and regularization)\n",
    "             # will be truned off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bbcd1876",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "82703027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.8701\n",
      "Precision:  0.8740288783074267\n",
      "Recall:  0.8701\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    outputs = model(X_test_tensor)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    \n",
    "    Y_test = Y_test_tensor.cpu().numpy()\n",
    "    predicted = predicted.cpu()\n",
    "    \n",
    "    print(\"Accuracy: \", accuracy_score(predicted, Y_test))\n",
    "    print(\"Precision: \", precision_score(predicted, Y_test, average='weighted'))\n",
    "    print(\"Recall: \", recall_score(predicted, Y_test, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74664d3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchenv",
   "language": "python",
   "name": "torchenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
